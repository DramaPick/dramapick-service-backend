import numpy as np
import os
import cv2
from dotenv import load_dotenv
import dlib
import face_recognition
from imutils import face_utils
from numpy.linalg import norm
from sklearn.cluster import AgglomerativeClustering
from s3_client import s3_client
import re
import io
import multiprocessing

load_dotenv()

TEMP_DIR = 'tmp'

# S3 ì„¤ì •
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_KEY")
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME", "test-fastapi-bucket")
S3_REGION_NAME = os.getenv("S3_REGION_NAME", "ap-northeast-2")

# dlibì˜ ì–¼êµ´ ê°ì§€ê¸° ë° ëœë“œë§ˆí¬ ì˜ˆì¸¡ê¸° ë¡œë“œ
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ

def delete_file(file_path):
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"íŒŒì¼ {file_path}ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"íŒŒì¼ {file_path}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì–¼êµ´ì—ì„œ ëˆˆ ê°ê¸° ë¹„ìœ¨ ê³„ì‚°
def eye_aspect_ratio(eye_points):
    A = np.linalg.norm(eye_points[1] - eye_points[5])
    B = np.linalg.norm(eye_points[2] - eye_points[4])
    C = np.linalg.norm(eye_points[0] - eye_points[3])
    ear = (A + B) / (2.0 * C)  # EAR ê³„ì‚°
    return ear

# í™”ì§ˆ ë„ˆë¬´ ì¢‹ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ê°ì§€ í•¨ìˆ˜
def getBlurScore(image):
    if len(image.shape) == 3:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    return cv2.Laplacian(image, cv2.CV_64F).var()

# ì–¼êµ´ì—ì„œ ëˆˆ ê°ê¸° ë° ì›ƒìŒ ê°ì§€
def detect_smiling_and_eye(frame, faces, frame_number, frame_width, frame_height, task_id):
    for i, face in enumerate(faces):
        x1, y1, x2, y2 = (face.left(), face.top(), face.right(), face.bottom())
        face_width = x2 - x1
        face_height = y2 - y1

        if face_width < 500 or face_height < 500:
            continue

        margin_x = int(face_width * 0.1)
        margin_y = int(face_height * 0.1)
        new_x1 = max(x1 - margin_x, 0)
        new_y1 = max(y1 - margin_y, 0)
        new_x2 = min(x2 + margin_x, frame_width)
        new_y2 = min(y2 + margin_y, frame_height)

        face_image_ = frame[new_y1:new_y2, new_x1:new_x2]
        if getBlurScore(face_image_) < 5:
            continue  # íë¦¿í•œ ì–¼êµ´ ê±´ë„ˆë›°ê¸°

        gray_face_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        landmarks = predictor(gray_face_image, face)
        landmarks = face_utils.shape_to_np(landmarks)

        left_eye = landmarks[36:42]
        right_eye = landmarks[42:48]
        left_ear = eye_aspect_ratio(np.array(left_eye))
        right_ear = eye_aspect_ratio(np.array(right_eye))
        ear = (left_ear + right_ear) / 2.0

        lip_jaw_ratio = norm(landmarks[54] - landmarks[48]) / norm(landmarks[2] - landmarks[14])
        mouth_opening = norm(landmarks[57] - landmarks[51])
        mouth_nose = norm(landmarks[33] - landmarks[51])

        is_smiling = lip_jaw_ratio > 0.44 and mouth_opening / mouth_nose >= 1.05

        if ear < 0.2:
            if is_smiling:
                filename = os.path.join(TEMP_DIR, f"v{task_id}_frame_{frame_number}_face_{i + 1}_smiling_and_eye_closed.jpg")
                cv2.imwrite(filename, frame[new_y1:new_y2, new_x1:new_x2])
            else:
                continue
        else:
            filename = os.path.join(
                TEMP_DIR, f"v{task_id}_frame_{frame_number}_face_{i + 1}.jpg")
            cv2.imwrite(filename, frame[new_y1:new_y2, new_x1:new_x2])

# ì–¼êµ´ ê°ë„ ê³„ì‚° í•¨ìˆ˜
def calculate_face_angle(image_path):
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    face_landmarks_list = face_recognition.face_landmarks(image_rgb)

    if len(face_landmarks_list) > 0:
        face_landmarks = face_landmarks_list[0]
        left_eye = face_landmarks['left_eye']
        right_eye = face_landmarks['right_eye']
        nose = face_landmarks['nose_bridge']

        eye_distance = np.linalg.norm(
            np.array(left_eye[0]) - np.array(right_eye[3]))
        nose_to_eyes_distance = np.linalg.norm(
            np.array(nose[0]) - np.array(left_eye[0]))
        angle = np.arctan(nose_to_eyes_distance / eye_distance)
        angle_deg = np.degrees(angle)
        return angle_deg
    else:
        return float('inf')


def parse_s3_url(s3_url: str):
    regex = r"https://([^/]+)\.s3\.[^/]+\.amazonaws\.com/(.+)"
    match = re.match(regex, s3_url)
    if not match:
        raise ValueError(f"Invalid S3 URL: {s3_url}")
    bucket_name = match.group(1)
    object_key = match.group(2)
    print(f"bucket_name : {bucket_name}, object_key: {object_key}")
    return bucket_name, object_key

def get_from_s3(s3_url: str):
    bucket_name, object_key = parse_s3_url(s3_url)
    # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    s3_object = s3_client.get_object(Bucket=bucket_name, Key=object_key)
    # íŒŒì¼ ì½˜í…ì¸ ë¥¼ ë©”ëª¨ë¦¬ ìƒì— ì €ì¥
    video_data = s3_object['Body'].read()  # ì´ ë°ì´í„°ëŠ” BytesIO í˜•íƒœë¡œ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # BytesIOë¡œ ë³€í™˜í•˜ì—¬ OpenCVë‚˜ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡
    video_stream = io.BytesIO(video_data)
    return video_stream

# ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
def extract_face_features(image_path):
    if ".mp4" in image_path or ".mov" in image_path:
        return None, None
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"Failed to read image at {image_path}. Please check the file path or format.")
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    encodings = face_recognition.face_encodings(image_rgb)

    if len(encodings) > 0:
        return encodings[0], image_path
    else:
        return None, image_path

# ë³‘ë ¬í™”ëœ ì–¼êµ´ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
def extract_face_features_parallel(image_paths):
    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        results = pool.map(extract_face_features, image_paths)

    # ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§
    features = []
    filenames = []
    for encoding, filename in results:
        if encoding is None and filename is None:
            continue
        elif encoding is not None:
            features.append(encoding)
            filenames.append(filename)
        else:
            print(f"ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename}")

    return features, filenames

# ë³‘ë ¬í™”ëœ ì–¼êµ´ ê°ë„ ê³„ì‚° í•¨ìˆ˜
def calculate_face_angle_parallel(image_paths):
    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        angles = pool.map(calculate_face_angle, image_paths)
    return angles


def face_detection_and_clustering(s3_url, task_id):
    _, object_key = parse_s3_url(s3_url)
    filename = object_key.split('/')[-1]
    base_name = filename.split('.')[0]  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
    if "mov" in s3_url:
        local_path = os.path.join("tmp", f"{base_name}.mov")  # ëª…ì‹œì ìœ¼ë¡œ .mov í™•ì¥ì ì§€ì •
    elif "mp4" in s3_url:
        local_path = os.path.join("tmp", f"{base_name}.mp4")  # ëª…ì‹œì ìœ¼ë¡œ .mp4 í™•ì¥ì ì§€ì • 
    print(f"â³ ------------ FACE DETECTION AND CLUSTERING -> local_path : {local_path} ------------")


    cap = cv2.VideoCapture(local_path)
    fps = cap.get(cv2.CAP_PROP_FPS)

    frame_interval = (int(fps) - 1) * 1
    frame_number = 0

    print(f"â³ frame interval: {frame_interval}")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_number += 1
        if frame_number % frame_interval == 0:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = detector(gray)
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            detect_smiling_and_eye(frame, faces, frame_number, frame_width, frame_height, task_id)

    cap.release()

    print("â˜ï¸ í”„ë ˆì„ ì €ì¥ ì™„ë£Œ â˜ï¸")

    # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡
    image_files = os.listdir(TEMP_DIR)
    image_paths = []
    for file in image_files:
        if "mov" in os.path.join(TEMP_DIR, file):
            pass
        elif "mp4" in os.path.join(TEMP_DIR, file):
            pass
        else:
            image_paths.append(os.path.join(TEMP_DIR, file))
    
    features, filenames = extract_face_features_parallel(image_paths)

    print("â˜ï¸ ì´ë¯¸ì§€ ê²½ë¡œ mov ì œì™¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ ì™„ë£Œ â˜ï¸")
    if len(features) == 0:
        print("ì–¼êµ´ ì¸ì‹ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ êµ°ì§‘í™”
    print("ğŸª« ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ êµ°ì§‘í™” í•™ìŠµ ì‹œì‘")
    clustering = AgglomerativeClustering(distance_threshold=0.05, n_clusters=None, metric='cosine', linkage='average')
    clustering.fit(features)

    print("ğŸª« ëŒ€í‘œ ì´ë¯¸ì§€ ì„ ì • ì¤€ë¹„")
    representative_images = []
    angles = calculate_face_angle_parallel(filenames)
    
    # ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ ì •
    for cluster_id in np.unique(clustering.labels_):
        print(f"Cluster {cluster_id}:")
        cluster_indices = np.where(clustering.labels_ == cluster_id)[0]
        representative_image = None
        min_angle = float('inf')

        for idx in cluster_indices:
            # angle = calculate_face_angle(filenames[idx])
            angle = angles[idx]
            if angle < min_angle:
                min_angle = angle
                representative_image = filenames[idx]

        print(f"Representative Image for Cluster {cluster_id}: {representative_image}")
        representative_images.append(representative_image)

        if representative_image:
            output_image_path = os.path.join(TEMP_DIR, f"v{task_id}_cluster_{cluster_id}_representative.jpg")
            img = cv2.imread(representative_image)
            cv2.imwrite(output_image_path, img)

    print(f"í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. --> {representative_images}")
    return representative_images